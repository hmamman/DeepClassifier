{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89ff3aa-16f8-4a8d-a3b7-854926527139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Bidirectional, Dense, Dropout, BatchNormalization, Concatenate, Flatten\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54925b1f-485e-45cc-9883-7ee7e6b9c7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>HDL Chol</th>\n",
       "      <th>Chol/HDL ratio</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Systolic BP</th>\n",
       "      <th>Diastolic BP</th>\n",
       "      <th>waist</th>\n",
       "      <th>hip</th>\n",
       "      <th>Waist/hip ratio</th>\n",
       "      <th>Diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193</td>\n",
       "      <td>77</td>\n",
       "      <td>49</td>\n",
       "      <td>3.9</td>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>61</td>\n",
       "      <td>119</td>\n",
       "      <td>22.5</td>\n",
       "      <td>118</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>0.84</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146</td>\n",
       "      <td>79</td>\n",
       "      <td>41</td>\n",
       "      <td>3.6</td>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>60</td>\n",
       "      <td>135</td>\n",
       "      <td>26.4</td>\n",
       "      <td>108</td>\n",
       "      <td>58</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>0.83</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>217</td>\n",
       "      <td>75</td>\n",
       "      <td>54</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>67</td>\n",
       "      <td>187</td>\n",
       "      <td>29.3</td>\n",
       "      <td>110</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226</td>\n",
       "      <td>97</td>\n",
       "      <td>70</td>\n",
       "      <td>3.2</td>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>114</td>\n",
       "      <td>19.6</td>\n",
       "      <td>122</td>\n",
       "      <td>64</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>0.79</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164</td>\n",
       "      <td>91</td>\n",
       "      <td>67</td>\n",
       "      <td>2.4</td>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>70</td>\n",
       "      <td>141</td>\n",
       "      <td>20.2</td>\n",
       "      <td>122</td>\n",
       "      <td>86</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>0.82</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>227</td>\n",
       "      <td>105</td>\n",
       "      <td>44</td>\n",
       "      <td>5.2</td>\n",
       "      <td>83</td>\n",
       "      <td>female</td>\n",
       "      <td>59</td>\n",
       "      <td>125</td>\n",
       "      <td>25.2</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>0.88</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>226</td>\n",
       "      <td>279</td>\n",
       "      <td>52</td>\n",
       "      <td>4.3</td>\n",
       "      <td>84</td>\n",
       "      <td>female</td>\n",
       "      <td>60</td>\n",
       "      <td>192</td>\n",
       "      <td>37.5</td>\n",
       "      <td>144</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>Diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>301</td>\n",
       "      <td>90</td>\n",
       "      <td>118</td>\n",
       "      <td>2.6</td>\n",
       "      <td>89</td>\n",
       "      <td>female</td>\n",
       "      <td>61</td>\n",
       "      <td>115</td>\n",
       "      <td>21.7</td>\n",
       "      <td>218</td>\n",
       "      <td>90</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>0.76</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>232</td>\n",
       "      <td>184</td>\n",
       "      <td>114</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>61</td>\n",
       "      <td>127</td>\n",
       "      <td>24.0</td>\n",
       "      <td>170</td>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>165</td>\n",
       "      <td>94</td>\n",
       "      <td>69</td>\n",
       "      <td>2.4</td>\n",
       "      <td>92</td>\n",
       "      <td>female</td>\n",
       "      <td>62</td>\n",
       "      <td>217</td>\n",
       "      <td>39.7</td>\n",
       "      <td>160</td>\n",
       "      <td>82</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1.00</td>\n",
       "      <td>No diabetes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cholesterol  Glucose  HDL Chol  Chol/HDL ratio  Age  Gender  Height  \\\n",
       "0            193       77        49             3.9   19  female      61   \n",
       "1            146       79        41             3.6   19  female      60   \n",
       "2            217       75        54             4.0   20  female      67   \n",
       "3            226       97        70             3.2   20  female      64   \n",
       "4            164       91        67             2.4   20  female      70   \n",
       "..           ...      ...       ...             ...  ...     ...     ...   \n",
       "385          227      105        44             5.2   83  female      59   \n",
       "386          226      279        52             4.3   84  female      60   \n",
       "387          301       90       118             2.6   89  female      61   \n",
       "388          232      184       114             2.0   91  female      61   \n",
       "389          165       94        69             2.4   92  female      62   \n",
       "\n",
       "     Weight   BMI  Systolic BP  Diastolic BP  waist  hip  Waist/hip ratio  \\\n",
       "0       119  22.5          118            70     32   38             0.84   \n",
       "1       135  26.4          108            58     33   40             0.83   \n",
       "2       187  29.3          110            72     40   45             0.89   \n",
       "3       114  19.6          122            64     31   39             0.79   \n",
       "4       141  20.2          122            86     32   39             0.82   \n",
       "..      ...   ...          ...           ...    ...  ...              ...   \n",
       "385     125  25.2          150            90     35   40             0.88   \n",
       "386     192  37.5          144            88     41   48             0.85   \n",
       "387     115  21.7          218            90     31   41             0.76   \n",
       "388     127  24.0          170            82     35   38             0.92   \n",
       "389     217  39.7          160            82     51   51             1.00   \n",
       "\n",
       "        Diabetes  \n",
       "0    No diabetes  \n",
       "1    No diabetes  \n",
       "2    No diabetes  \n",
       "3    No diabetes  \n",
       "4    No diabetes  \n",
       "..           ...  \n",
       "385  No diabetes  \n",
       "386     Diabetes  \n",
       "387  No diabetes  \n",
       "388     Diabetes  \n",
       "389  No diabetes  \n",
       "\n",
       "[390 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Datasets/informatics_edu.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ec2256-a71c-47fc-8c3c-31b8150de229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 390 entries, 0 to 389\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Cholesterol      390 non-null    int64  \n",
      " 1   Glucose          390 non-null    int64  \n",
      " 2   HDL Chol         390 non-null    int64  \n",
      " 3   Chol/HDL ratio   390 non-null    float64\n",
      " 4   Age              390 non-null    int64  \n",
      " 5   Gender           390 non-null    object \n",
      " 6   Height           390 non-null    int64  \n",
      " 7   Weight           390 non-null    int64  \n",
      " 8   BMI              390 non-null    float64\n",
      " 9   Systolic BP      390 non-null    int64  \n",
      " 10  Diastolic BP     390 non-null    int64  \n",
      " 11  waist            390 non-null    int64  \n",
      " 12  hip              390 non-null    int64  \n",
      " 13  Waist/hip ratio  390 non-null    float64\n",
      " 14  Diabetes         390 non-null    object \n",
      "dtypes: float64(3), int64(10), object(2)\n",
      "memory usage: 45.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3bba48-6c9b-45f0-a9eb-9795ed3fb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(columns=['Diabetes'])\n",
    "y = df['Diabetes']\n",
    "\n",
    "# Encode categorical features\n",
    "# Gender\n",
    "X['Gender'] = X['Gender'].map({'male': 1, 'memale': 0})\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Positive -> 1, Negative -> 0\n",
    "\n",
    "# Handle missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e56d99-f4b5-4231-a2ad-47604eeb0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "X_smote, y_smote = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca015ecc-082a-4864-addc-96324fb5b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (avoid scaling entire data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621ecd4f-3561-4035-b937-6bb905107f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM\n",
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "X_lstm = np.reshape(X, (X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce28990-5191-47c0-9152-d046e8c478b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your input data is in the format (samples, features)\n",
    "# Define the inputs\n",
    "inputs = Input(shape=(X_lstm.shape[1], 1))\n",
    "\n",
    "# Dense layers\n",
    "x = Dense(128, activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Recurrent layers\n",
    "x_lstm = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x_gru = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "# Concatenate LSTM and GRU outputs\n",
    "x = Concatenate()([x_lstm, x_gru])\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense layers\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e370eb-f6bb-49b5-bfda-23546039d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9783 - loss: 0.0817 - val_accuracy: 1.0000 - val_loss: 0.0047\n",
      "Epoch 2/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9940 - loss: 0.0247 - val_accuracy: 0.9906 - val_loss: 0.0171\n",
      "Epoch 3/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9795 - loss: 0.0392 - val_accuracy: 0.9906 - val_loss: 0.0089\n",
      "Epoch 4/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9912 - loss: 0.0399 - val_accuracy: 1.0000 - val_loss: 0.0059\n",
      "Epoch 5/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9907 - loss: 0.0445 - val_accuracy: 0.9906 - val_loss: 0.0126\n",
      "Epoch 6/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9811 - loss: 0.0694 - val_accuracy: 1.0000 - val_loss: 0.0218\n",
      "Epoch 7/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9758 - loss: 0.0531 - val_accuracy: 1.0000 - val_loss: 0.0125\n",
      "Epoch 8/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9683 - loss: 0.0737 - val_accuracy: 0.9717 - val_loss: 0.0810\n",
      "Epoch 9/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9786 - loss: 0.0603 - val_accuracy: 0.9811 - val_loss: 0.0519\n",
      "Epoch 10/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9882 - loss: 0.0431 - val_accuracy: 0.9811 - val_loss: 0.0540\n",
      "Epoch 11/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9782 - loss: 0.0659 - val_accuracy: 0.9811 - val_loss: 0.0794\n",
      "Epoch 12/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9804 - loss: 0.0436 - val_accuracy: 0.9906 - val_loss: 0.0177\n",
      "Epoch 13/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9897 - loss: 0.0371 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
      "Epoch 14/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9876 - loss: 0.0362 - val_accuracy: 0.9717 - val_loss: 0.0558\n",
      "Epoch 15/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9841 - loss: 0.0366 - val_accuracy: 0.9811 - val_loss: 0.0455\n",
      "Epoch 16/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9832 - loss: 0.0338 - val_accuracy: 0.9906 - val_loss: 0.0212\n",
      "Epoch 17/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9960 - loss: 0.0234 - val_accuracy: 1.0000 - val_loss: 0.0157\n",
      "Epoch 18/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9689 - loss: 0.0695 - val_accuracy: 0.9906 - val_loss: 0.0128\n",
      "Epoch 19/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9980 - loss: 0.0179 - val_accuracy: 0.9906 - val_loss: 0.0174\n",
      "Epoch 20/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9789 - loss: 0.0571 - val_accuracy: 0.9623 - val_loss: 0.1073\n",
      "Epoch 21/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9893 - loss: 0.0399 - val_accuracy: 0.9906 - val_loss: 0.0311\n",
      "Epoch 22/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9881 - loss: 0.0370 - val_accuracy: 0.9717 - val_loss: 0.1342\n",
      "Epoch 23/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9890 - loss: 0.0290 - val_accuracy: 0.9717 - val_loss: 0.1297\n",
      "Epoch 24/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9980 - loss: 0.0183 - val_accuracy: 0.9906 - val_loss: 0.0264\n",
      "Epoch 25/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 0.9906 - val_loss: 0.0444\n",
      "Epoch 26/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9915 - loss: 0.0254 - val_accuracy: 0.9906 - val_loss: 0.0199\n",
      "Epoch 27/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9985 - loss: 0.0113 - val_accuracy: 0.9811 - val_loss: 0.0181\n",
      "Epoch 28/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9835 - loss: 0.0311 - val_accuracy: 0.9906 - val_loss: 0.0117\n",
      "Epoch 29/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9918 - loss: 0.0272 - val_accuracy: 0.9906 - val_loss: 0.0126\n",
      "Epoch 30/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9880 - loss: 0.0210 - val_accuracy: 1.0000 - val_loss: 0.0084\n",
      "Epoch 31/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9940 - loss: 0.0129 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
      "Epoch 32/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9972 - loss: 0.0180 - val_accuracy: 0.9528 - val_loss: 0.1875\n",
      "Epoch 33/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9951 - loss: 0.0173 - val_accuracy: 0.9717 - val_loss: 0.1017\n",
      "Epoch 34/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9968 - loss: 0.0155 - val_accuracy: 0.9717 - val_loss: 0.0710\n",
      "Epoch 35/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9885 - loss: 0.0406 - val_accuracy: 0.9434 - val_loss: 0.1660\n",
      "Epoch 36/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9805 - loss: 0.0621 - val_accuracy: 0.9623 - val_loss: 0.0886\n",
      "Epoch 37/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9897 - loss: 0.0284 - val_accuracy: 0.9623 - val_loss: 0.0971\n",
      "Epoch 38/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9771 - loss: 0.0493 - val_accuracy: 0.9906 - val_loss: 0.0461\n",
      "Epoch 39/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9894 - loss: 0.0275 - val_accuracy: 1.0000 - val_loss: 0.0135\n",
      "Epoch 40/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9994 - loss: 0.0129 - val_accuracy: 0.9906 - val_loss: 0.0130\n",
      "Epoch 41/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9922 - loss: 0.0226 - val_accuracy: 0.9906 - val_loss: 0.0141\n",
      "Epoch 42/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9850 - loss: 0.0404 - val_accuracy: 0.9811 - val_loss: 0.0412\n",
      "Epoch 43/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9734 - loss: 0.0688 - val_accuracy: 0.9717 - val_loss: 0.0556\n",
      "Epoch 44/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9785 - loss: 0.0587 - val_accuracy: 0.9623 - val_loss: 0.0635\n",
      "Epoch 45/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9861 - loss: 0.0514 - val_accuracy: 0.9811 - val_loss: 0.0547\n",
      "Epoch 46/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9959 - loss: 0.0224 - val_accuracy: 0.9811 - val_loss: 0.0416\n",
      "Epoch 47/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9936 - loss: 0.0234 - val_accuracy: 0.9811 - val_loss: 0.0331\n",
      "Epoch 48/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.9906 - val_loss: 0.0267\n",
      "Epoch 49/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9995 - loss: 0.0077 - val_accuracy: 0.9906 - val_loss: 0.0251\n",
      "Epoch 50/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.0304 - val_accuracy: 0.9906 - val_loss: 0.0271\n",
      "Epoch 51/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9878 - loss: 0.0341 - val_accuracy: 0.9906 - val_loss: 0.0275\n",
      "Epoch 52/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9927 - loss: 0.0242 - val_accuracy: 1.0000 - val_loss: 0.0054\n",
      "Epoch 53/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9754 - loss: 0.0388 - val_accuracy: 0.9906 - val_loss: 0.0124\n",
      "Epoch 54/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9921 - loss: 0.0196 - val_accuracy: 0.9906 - val_loss: 0.0513\n",
      "Epoch 55/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9911 - loss: 0.0276 - val_accuracy: 0.9906 - val_loss: 0.0568\n",
      "Epoch 56/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9933 - loss: 0.0208 - val_accuracy: 0.9906 - val_loss: 0.0526\n",
      "Epoch 57/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.9906 - val_loss: 0.0516\n",
      "Epoch 58/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9974 - loss: 0.0109 - val_accuracy: 0.9906 - val_loss: 0.0571\n",
      "Epoch 59/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9959 - loss: 0.0164 - val_accuracy: 0.9906 - val_loss: 0.0656\n",
      "Epoch 60/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9926 - loss: 0.0239 - val_accuracy: 0.9717 - val_loss: 0.0770\n",
      "Epoch 61/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9893 - loss: 0.0245 - val_accuracy: 0.9811 - val_loss: 0.0888\n",
      "Epoch 62/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9920 - loss: 0.0190 - val_accuracy: 0.9717 - val_loss: 0.1058\n",
      "Epoch 63/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9622 - loss: 0.1203 - val_accuracy: 0.9717 - val_loss: 0.1512\n",
      "Epoch 64/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9855 - loss: 0.0471 - val_accuracy: 0.9623 - val_loss: 0.1623\n",
      "Epoch 65/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9913 - loss: 0.0292 - val_accuracy: 0.9623 - val_loss: 0.0774\n",
      "Epoch 66/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9980 - loss: 0.0235 - val_accuracy: 0.9717 - val_loss: 0.0481\n",
      "Epoch 67/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9922 - loss: 0.0288 - val_accuracy: 0.9623 - val_loss: 0.0707\n",
      "Epoch 68/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9919 - loss: 0.0293 - val_accuracy: 0.9528 - val_loss: 0.0921\n",
      "Epoch 69/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9841 - loss: 0.0356 - val_accuracy: 0.9623 - val_loss: 0.0773\n",
      "Epoch 70/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9838 - loss: 0.0506 - val_accuracy: 0.9717 - val_loss: 0.0995\n",
      "Epoch 71/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9646 - loss: 0.0811 - val_accuracy: 0.9906 - val_loss: 0.0214\n",
      "Epoch 72/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9828 - loss: 0.0357 - val_accuracy: 0.9811 - val_loss: 0.0280\n",
      "Epoch 73/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9929 - loss: 0.0410 - val_accuracy: 0.9811 - val_loss: 0.0357\n",
      "Epoch 74/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9715 - loss: 0.0819 - val_accuracy: 0.9057 - val_loss: 0.2742\n",
      "Epoch 75/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9917 - loss: 0.0286 - val_accuracy: 0.9434 - val_loss: 0.1480\n",
      "Epoch 76/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9872 - loss: 0.0432 - val_accuracy: 0.9623 - val_loss: 0.0785\n",
      "Epoch 77/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0175 - val_accuracy: 0.9623 - val_loss: 0.0513\n",
      "Epoch 78/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9923 - loss: 0.0279 - val_accuracy: 0.9623 - val_loss: 0.0455\n",
      "Epoch 79/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9890 - loss: 0.0425 - val_accuracy: 0.9811 - val_loss: 0.0402\n",
      "Epoch 80/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9952 - loss: 0.0223 - val_accuracy: 0.9717 - val_loss: 0.0549\n",
      "Epoch 81/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9822 - loss: 0.0373 - val_accuracy: 0.9906 - val_loss: 0.0279\n",
      "Epoch 82/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9962 - loss: 0.0192 - val_accuracy: 0.9906 - val_loss: 0.0363\n",
      "Epoch 83/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9862 - loss: 0.0331 - val_accuracy: 0.9811 - val_loss: 0.0278\n",
      "Epoch 84/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9939 - loss: 0.0176 - val_accuracy: 0.9717 - val_loss: 0.0387\n",
      "Epoch 85/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9946 - loss: 0.0194 - val_accuracy: 0.9811 - val_loss: 0.0394\n",
      "Epoch 86/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9900 - loss: 0.0231 - val_accuracy: 0.9906 - val_loss: 0.0138\n",
      "Epoch 87/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9880 - loss: 0.0375 - val_accuracy: 0.9811 - val_loss: 0.0309\n",
      "Epoch 88/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 0.0182 - val_accuracy: 0.9717 - val_loss: 0.0534\n",
      "Epoch 89/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9843 - loss: 0.0438 - val_accuracy: 0.9811 - val_loss: 0.0779\n",
      "Epoch 90/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9701 - loss: 0.0714 - val_accuracy: 0.9811 - val_loss: 0.0633\n",
      "Epoch 91/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9958 - loss: 0.0230 - val_accuracy: 0.9811 - val_loss: 0.0423\n",
      "Epoch 92/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9933 - loss: 0.0239 - val_accuracy: 0.9717 - val_loss: 0.0968\n",
      "Epoch 93/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9924 - loss: 0.0215 - val_accuracy: 0.9717 - val_loss: 0.1008\n",
      "Epoch 94/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9932 - loss: 0.0278 - val_accuracy: 0.9528 - val_loss: 0.1356\n",
      "Epoch 95/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9890 - loss: 0.0384 - val_accuracy: 0.9811 - val_loss: 0.0772\n",
      "Epoch 96/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9863 - loss: 0.0510 - val_accuracy: 0.9623 - val_loss: 0.0695\n",
      "Epoch 97/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9882 - loss: 0.0490 - val_accuracy: 0.9623 - val_loss: 0.0918\n",
      "Epoch 98/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9798 - loss: 0.0443 - val_accuracy: 0.9623 - val_loss: 0.0782\n",
      "Epoch 99/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9792 - loss: 0.0461 - val_accuracy: 0.9717 - val_loss: 0.0587\n",
      "Epoch 100/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9933 - loss: 0.0176 - val_accuracy: 0.9811 - val_loss: 0.0695\n",
      "Epoch 101/2000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9979 - loss: 0.0223 - val_accuracy: 0.9811 - val_loss: 0.0824\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9574 - loss: 0.0995 \n",
      "Test Loss: 0.0860\n",
      "Test Accuracy: 0.9621\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# Train the model\n",
    "history = model.fit(X_train_lstm, y_train, epochs=2000, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_lstm, y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c358232-7820-45fa-b293-d236e9f5411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a file\n",
    "model.save('Results/informatics/bilstm_model.keras')\n",
    "# Save only the model's weights to a file\n",
    "model.save_weights('Results/informatics/bilstm_model.weights.h5')\n",
    "# Save only the model's architecture to a JSON file\n",
    "model_json = model.to_json()\n",
    "with open('Results/informatics/bistlm_model_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save X_test as a NumPy array\n",
    "np.save('Results/informatics/bilstm_X_test.npy', X_test_lstm)\n",
    "\n",
    "# Save y_test as a NumPy array\n",
    "np.save('Results/informatics/bilstm_y_test.npy', y_test)\n",
    "\n",
    "# Get the training and validation accuracy and loss values\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Save the accuracy and loss values as NumPy arrays\n",
    "np.save('Results/informatics/bilstm_train_acc.npy', train_acc)\n",
    "np.save('Results/informatics/bilstm_val_acc.npy', val_acc)\n",
    "np.save('Results/informatics/bilstm_train_loss.npy', train_loss)\n",
    "np.save('Results/informatics/bilstm_val_loss.npy', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c539717-7bd4-4640-a54c-5e687352b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test) > 0.5\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix', fontsize=12, fontweight='normal')\n",
    "plt.xlabel('Predicted Label', fontsize=12, labelpad=10)\n",
    "plt.ylabel('True Label', fontsize=12, labelpad=10)\n",
    "plt.show()\n",
    "# Set the title and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72509e5-1c62-43e9-80ad-65d110540cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1303f-0354-4dc7-bab8-b030f2ffa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Set background color and grid style for all subplots\n",
    "fig.set_facecolor('white')\n",
    "for ax in axes.flat:\n",
    "    ax.grid(color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], linewidth=2, color='#1f77b4', label='Training set')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], linewidth=2, color='#ff7f0e', label='Testing set')\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=12, fontweight='normal')\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].legend(loc='lower right', fontsize=10)\n",
    "axes[0, 0].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Plot loss\n",
    "axes[0, 1].plot(history.history['loss'], linewidth=2, color='#1f77b4', label='Training set')\n",
    "axes[0, 1].plot(history.history['val_loss'], linewidth=2, color='#ff7f0e', label='Testing set')\n",
    "axes[0, 1].set_title('Model Loss', fontsize=12, fontweight='normal')\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].legend(loc='upper right', fontsize=10)\n",
    "axes[0, 1].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Compute the ROC curve and AUC score\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "axes[1, 0].plot(fpr, tpr, color='#ff7f0e', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='#1f77b4', linewidth=2, linestyle='--', label='Random guess')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1, 0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1, 0].set_title('ROC Curve', fontsize=12, fontweight='normal')\n",
    "axes[1, 0].tick_params(axis='both', which='major', labelsize=10)\n",
    "axes[1, 0].legend(loc='lower right', fontsize=10)\n",
    "\n",
    "# Compute Precision-Recall curve and average precision score\n",
    "y_score = model.predict(X_test)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "ap = average_precision_score(y_test, y_score)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "axes[1, 1].plot(recall, precision, color='#ff7f0e', linewidth=2, label=f'Precision-Recall curve (AP = {ap:.2f})')\n",
    "axes[1, 1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1, 1].set_title('Precision-Recall Curve', fontsize=12, fontweight='normal')\n",
    "axes[1, 1].tick_params(axis='both', which='major', labelsize=10)\n",
    "axes[1, 1].legend(loc=\"lower left\", fontsize=10)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea8ec3-6737-4995-a807-ca9f468d2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_plot(test_x_scaled, test_y, test_y_pred, size=100, markersize=1):\n",
    "    pca = PCA(n_components=2)\n",
    "    x_pca = pca.fit_transform(test_x_scaled)\n",
    "\n",
    "    true_np = np.hstack((x_pca, test_y.reshape(len(test_y), 1)))\n",
    "    pred_np = np.hstack((x_pca, test_y_pred.reshape(len(test_y_pred), 1)))\n",
    "\n",
    "    column_true = ['pc_0', 'pc_1', 'true']\n",
    "    column_pred = ['pc_0', 'pc_1', 'pred']\n",
    "\n",
    "    true_df = pd.DataFrame(data=true_np, columns=column_true)\n",
    "    pred_df = pd.DataFrame(data=pred_np, columns=column_pred)\n",
    "\n",
    "    true_df_0 = true_df[true_df['true'] == 0]\n",
    "    true_df_1 = true_df[true_df['true'] == 1]\n",
    "    true_x_0 = true_df_0.iloc[:, :-1].to_numpy()\n",
    "    true_x_1 = true_df_1.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    pred_df_0 = pred_df[pred_df['pred'] == 0]\n",
    "    pred_df_1 = pred_df[pred_df['pred'] == 1]\n",
    "    pred_x_0 = pred_df_0.iloc[:, :-1].to_numpy()\n",
    "    pred_x_1 = pred_df_1.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    join_df = true_df.join(pred_df['pred'], rsuffix='_pred')\n",
    "\n",
    "    correct_df = join_df[join_df['true'] == join_df['pred']]\n",
    "    wrong_df = join_df[join_df['true'] != join_df['pred']]\n",
    "\n",
    "    correct_df_0 = correct_df[correct_df['true'] == 0]\n",
    "    correct_df_1 = correct_df[correct_df['true'] == 1]\n",
    "    correct_x_0 = correct_df_0.iloc[:, :-2].to_numpy()\n",
    "    correct_x_1 = correct_df_1.iloc[:, :-2].to_numpy()\n",
    "\n",
    "    fn_df = wrong_df[wrong_df['pred'] == 0]\n",
    "    fp_df = wrong_df[wrong_df['pred'] == 1]\n",
    "    fn_x = fn_df.iloc[:, :-2].to_numpy()\n",
    "    fp_x = fp_df.iloc[:, :-2].to_numpy()\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    ax1.scatter(true_x_0[:, 0], true_x_0[:, 1], c='dodgerblue', s=size, alpha=0.6, label='Negative Cases')\n",
    "    ax1.scatter(true_x_1[:, 0], true_x_1[:, 1], c='orange', s=size, alpha=0.6, label='Positive Cases')\n",
    "    ax1.legend(markerscale=markersize, loc='best', frameon=True, facecolor='white', edgecolor='black')\n",
    "    ax1.set_title('Ground Truth of Test Data', fontsize=12)\n",
    "    # ax1.set_xlabel('Principal Component 1', fontsize=12)\n",
    "    # ax1.set_ylabel('Principal Component 2', fontsize=12)\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Predictions\n",
    "    ax2.scatter(pred_x_0[:, 0], pred_x_0[:, 1], c='dodgerblue', s=size, alpha=0.6, label='Negative Cases')\n",
    "    ax2.scatter(pred_x_1[:, 0], pred_x_1[:, 1], c='orange', s=size, alpha=0.6, label='Positive Cases')\n",
    "    ax2.legend(markerscale=markersize, loc='best', frameon=True, facecolor='white', edgecolor='black')\n",
    "    ax2.set_title('Prediction', fontsize=12)\n",
    "    # ax2.set_xlabel('Principal Component 1', fontsize=12)\n",
    "    # ax2.set_ylabel('Principal Component 2', fontsize=12)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot Comparison\n",
    "    ax3.scatter(correct_x_0[:, 0], correct_x_0[:, 1], c='dodgerblue', s=size, alpha=0.6, label='True Negative')\n",
    "    ax3.scatter(correct_x_1[:, 0], correct_x_1[:, 1], c='orange', s=size, alpha=0.6, label='True Positive')\n",
    "    ax3.scatter(fn_x[:, 0], fn_x[:, 1], c='limegreen', s=size, alpha=0.6, label='False Negative')\n",
    "    ax3.scatter(fp_x[:, 0], fp_x[:, 1], c='red', s=size, alpha=0.6, label='False Positive')\n",
    "    ax3.legend(markerscale=markersize, loc='best', frameon=True, facecolor='white', edgecolor='black')\n",
    "    ax3.set_title('Comparison', fontsize=12)\n",
    "    # ax3.set_xlabel('Principal Component 1', fontsize=12)\n",
    "    # ax3.set_ylabel('Principal Component 2', fontsize=12)\n",
    "    ax3.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "pca_plot(X_test, y_test, y_pred, size=100, markersize=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff06bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
